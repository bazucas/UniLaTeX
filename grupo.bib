@article{haidt2001,
  author       = {Haidt, J.},
  title        = {The Emotional Dog and its Rational Tail: A Social Intuitionist Approach to Moral Judgment},
  journal      = {Psychological Review},
  volume       = {108},
  number       = {4},
  pages        = {814--834},
  year         = {2001},
  doi          = {10.1037/0033-295X.108.4.814}
}

@article{kaur2019,
  author = {Kaur, R. and Kautish, S.},
  title = {Multimodal Sentiment Analysis: A Survey and Comparison},
  journal = {International Journal of Service Science, Management, Engineering, and Technology (IJSSMET)},
  volume = {10},
  number = {2},
  pages = {38-58},
  year = {2019},
  doi = {10.4018/IJSSMET.2019040103},
  url = {https://doi.org/10.4018/IJSSMET.2019040103}
}

@inproceedings{poria2015,
    title = "Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterance-level Multimodal Sentiment Analysis",
    author = "Poria, Soujanya and Cambria, Erik  and Gelbukh, Alexander",
    editor = "M{\`a}rquez, Llu{\'\i}s  and Callison-Burch, Chris  and Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1303",
    doi = "10.18653/v1/D15-1303",
    pages = "2539--2544",
}

@book{picard1997,
  author       = {Picard, R. W.},
  title        = {Affective Computing},
  publisher    = {MIT Press},
  year         = {1997},
  address      = {Cambridge, MA}
}

@online{mueller2020,
  author       = {MÃ¼ller, V. C.},
  title        = {The Ethics of Artificial Intelligence and Robotics},
  year         = {2020},
  url          = {https://plato.stanford.edu/entries/ethics-ai/},
  note         = {Accessed: 2024-04-27}
}

@article{kargarandehkordi2024,
  author       = {Kargarandehkordi, A. and Kaisti, M. and Washington, P.},
  title        = {Personalization of Affective Models Using Classical Machine Learning: A Feasibility Study},
  journal      = {Applied Sciences},
  volume       = {14},
  number       = {1337},
  year         = {2024},
  doi          = {10.3390/app14041337}
}

@article{gursesli2024,
  author={Gursesli, Mustafa Can and Lombardi, Sara and Duradoni, Mirko and Bocchi, Leonardo and Guazzini, Andrea and Lanata, Antonio},
  journal={IEEE Access}, 
  title={Facial Emotion Recognition (FER) Through Custom Lightweight CNN Model: Performance Evaluation in Public Datasets}, 
  year={2024},
  volume={12},
  number={},
  pages={45543-45559},
  keywords={Computational modeling;Convolutional neural networks;Computer architecture;Emotion recognition;Face recognition;Computer vision;Brain modeling;Deep learning;Performance evaluation;Facial emotion recognition;convolutional neural networks;computer vision;emotion recognition;deep learning},
  doi={10.1109/ACCESS.2024.3380847}
}

@article{lee2024,
  author={Lee, Min-Ho and Shomanov, Adai and Begim, Balgyn and Kabidenova, Zhuldyz and Nyssanbay, Aruna and Yazici, Adnan and Lee, Seong-Whan},
  title={EAV: EEG-Audio-Video Dataset for Emotion Recognition in Conversational Contexts},
  journal={Scientific Data},
  year={2024},
  month={Sep},
  day={19},
  volume={11},
  number={1},
  pages={1026},
  abstract={Understanding emotional states is pivotal for the development of next-generation human-machine interfaces. Human behaviors in social interactions have resulted in psycho-physiological processes influenced by perceptual inputs. Therefore, efforts to comprehend brain functions and human behavior could potentially catalyze the development of AI models with human-like attributes. In this study, we introduce a multimodal emotion dataset comprising data from 30-channel electroencephalography (EEG), audio, and video recordings from 42 participants. Each participant engaged in a cue-based conversation scenario, eliciting five distinct emotions: neutral, anger, happiness, sadness, and calmness. Throughout the experiment, each participant contributed 200 interactions, which encompassed both listening and speaking. This resulted in a cumulative total of 8,400 interactions across all participants. We evaluated the baseline performance of emotion recognition for each modality using established deep neural network (DNN) methods. The Emotion in EEG-Audio-Visual (EAV) dataset represents the first public dataset to incorporate three primary modalities for emotion recognition within a conversational context. We anticipate that this dataset will make significant contributions to the modeling of the human emotional process, encompassing both fundamental neuroscience and machine learning viewpoints.},
  issn={2052-4463},
  doi={10.1038/s41597-024-03838-4},
  url={https://doi.org/10.1038/s41597-024-03838-4}
}
